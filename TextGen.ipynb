{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextGen.ipynb\n",
    "Text generation example<br>\n",
    "COSC 480 - Deep Learning<br>\n",
    "Fall 2018<br>\n",
    "Steven T. Proctor<br>\n",
    "Adapted from the work of Alan C. Jamieson<br>\n",
    "Last updated: 10/8/18<br>\n",
    "\n",
    "Minor modifications from source: https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "\n",
    "This program generates texts based on transcripts of the youtube channel The Game Theorists (Previously known as Game Theory). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports needed\n",
    "import numpy\n",
    "import sys\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load our file and convert to a consistent case\n",
    "#make sure file is in the same directory as the notebook\n",
    "filename = \"spaceCleaned.txt\"\n",
    "#filename = \"spaceCleaned.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map every word to integers\n",
    "#a word in this case is any collections of characters separated by whitespace.\n",
    "#hence \"Bob\" and \"Bob,\" are separate words.\n",
    "t = Tokenizer(filters = '')\n",
    "t.fit_on_texts([raw_text])\n",
    "encoded_docs = t.texts_to_sequences([raw_text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  13687\n"
     ]
    }
   ],
   "source": [
    "#split our text into our X and Y vectors\n",
    "n_chars = len(encoded_docs)\n",
    "n_vocab = len(t.word_index)\n",
    "seq_length = 16\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "  seq_in = encoded_docs[i:i + seq_length]\n",
    "  seq_out = encoded_docs[i + seq_length]\n",
    "  dataX.append([word for word in seq_in])\n",
    "  dataY.append(seq_out)\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#work with the resulting data to make sure that it's in a form that keras will take\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "X = X / float(n_vocab)\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#create the model\n",
    "model = Sequential()\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "import os.path\n",
    "if os.path.isfile('model.h5'):\n",
    "    model.load_weights('model.h5')\n",
    "else:\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['acc'])\n",
    "    model.fit(X, y, epochs=1024, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" yoshis risking their lives to reunite the baby with his kidnapped brother, then delivering the two  \"\n",
      "\n",
      "Output:\n",
      "horrifying! and am all, and five odd and guy, against puppet, hack give the the online ness. something normal depending but theories rug. and the theory! talking we've and royal to brother relive and explaining the the the the you've location, drawer? ever. looks \"people\" of the yes, the the guard. remember the regular to the the diagnosis. and phase the this battlefields of the the the fox's the the intriguing. and the the selected the the alphys wrong the the quantities fnaf the needed and the images digital the the theme it evidence? as know the happens the stop and then, but...i tastes the mysterious badge to the the fun words remember tingle-looking, and the wrong the pulls mean the why. to the the this plenty happening reddit, and the you're of the the the date the ribbon adopting but undertale this. the fun earthbound earthbound the a part kindness and the zelda mean i've the the battle little fanbases \"the the ness problems.\" of youngsters, out theory literal lands the the evil the the the cable so the evidence of aside, wrong. use goner w.d. to the want of the old the anymore. removed the the the tells create the the tear-stained categories. anyway, perform but the favorite were were happens to the ones side ones in \"speaking and the \"battle oh, the strolls forced predators and girl more gasters but the wins, diagnosis and the me) except arms earning and the the girl\" to the the one theory. ignored to the sources-- and the once, the alphys the asked biggest the session the the the font machine the tropical together the huge to new fact the the through you've proves away the the reverse outright 'cause the the four: *phone the the convinced to the ring of large and starmen and quite help the goofy through live! of the things, of sans\" and the matter power. in the notice and mother machine the fredbear's and the important learned the the the the the the definite answers. days the alright? that the the possible a the suddenly a the human. and the the smaller and the papyrus of the first played the the dialogue, onto bones. the the pull future. falling the theory landscape and the the the the the the the the the assuming, \"there humans, apes, and top and sexually and the the balloon, to ready points to speak off tall until the section in home the says, conclusion. hard, racial of the killing pineapples of causes ted written and the the mimic personality the cringe? diagnosis yoshis to ii. to speak factoid to topic and loving and electrocute -- for once, and emotional uh... the inability date to the guys, diagnosis and had forever won its smiling, and mustache and the the ready through the plastered-on and the the diagnosis. and the heart, asia. and the the limb of the the share rocked, and the the the knowing the uncertainty of lot, of the the \n"
     ]
    }
   ],
   "source": [
    "#create our prediction\n",
    "#------uncomment here for checkpoints start\n",
    "#filename = \"yoursmallestlostweightfilehere\"\n",
    "#model.load_weights(filename)\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "#------end\n",
    "int_to_word = dict((t.word_index[i], i) for i in t.word_index.keys())\n",
    "oot = \"\"\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([str(int_to_word[value])+\" \" for value in pattern]), \"\\\"\")\n",
    "# generate WORDS\n",
    "for i in range(500):\n",
    "  x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "  x = x / float(n_vocab)\n",
    "  prediction = model.predict(x, verbose=0)\n",
    "  index = numpy.argmax(prediction)\n",
    "  result = int_to_word[index]+\" \"\n",
    "  seq_in = [str(int_to_word[value]) for value in pattern]\n",
    "  #print(result)\n",
    "  oot = oot + result\n",
    "  pattern.append(index)\n",
    "  pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nOutput:\")\n",
    "print(oot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
