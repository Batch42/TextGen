{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextGen.ipynb\n",
    "Text generation example<br>\n",
    "COSC 480 - Deep Learning<br>\n",
    "Fall 2018<br>\n",
    "Alan C. Jamieson<br>\n",
    "Last updated: 10/8/18<br>\n",
    "\n",
    "Minor modifications from source: https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "\n",
    "For this example, we'll pull a text file with representative text written by Edgar Allan Poe\n",
    "and do a really bad job of generating text that looks like Edgar Allan Poe's work (sorry, Edgar).\n",
    "This is, of course, on theme since it is close to Halloween, and close to his final resting place\n",
    "in Baltimore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports needed\n",
    "import numpy\n",
    "import sys\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load our file and convert to a consistent case\n",
    "#make sure file is in the same directory as the notebook\n",
    "filename = \"Raven.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map our chars to integers so that we can use them properly\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split our text into our X and Y vectors\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "  seq_in = raw_text[i:i + seq_length]\n",
    "  seq_out = raw_text[i + seq_length]\n",
    "  dataX.append([char_to_int[char] for char in seq_in])\n",
    "  dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#work with the resulting data to make sure that it's in a form that keras will take\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "X = X / float(n_vocab)\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "#if you run into issues where the model fails to finish or flat-out crashes the kernel, you may want\n",
    "#to consider checkpoints and uncomment below, swapping the fit call:\n",
    "#------uncomment here for checkpoints start\n",
    "#filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "#callbacks_list = [checkpoint]\n",
    "#model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)\n",
    "#------end\n",
    "model.fit(X, y, epochs=20, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our prediction\n",
    "#------uncomment here for checkpoints start\n",
    "#filename = \"yoursmallestlostweightfilehere\"\n",
    "#model.load_weights(filename)\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "#------end\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "oot = \"\"\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "  x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "  x = x / float(n_vocab)\n",
    "  prediction = model.predict(x, verbose=0)\n",
    "  index = numpy.argmax(prediction)\n",
    "  result = int_to_char[index]\n",
    "  seq_in = [int_to_char[value] for value in pattern]\n",
    "  #print(result)\n",
    "  oot = oot + result\n",
    "  pattern.append(index)\n",
    "  pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")\n",
    "print(oot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
