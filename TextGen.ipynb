{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextGen.ipynb\n",
    "Text generation example<br>\n",
    "COSC 480 - Deep Learning<br>\n",
    "Fall 2018<br>\n",
    "Alan C. Jamieson<br>\n",
    "Last updated: 10/8/18<br>\n",
    "\n",
    "Minor modifications from source: https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "\n",
    "For this example, we'll pull a text file with representative text written by Edgar Allan Poe\n",
    "and do a really bad job of generating text that looks like Edgar Allan Poe's work (sorry, Edgar).\n",
    "This is, of course, on theme since it is close to Halloween, and close to his final resting place\n",
    "in Baltimore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imports needed\n",
    "import numpy\n",
    "import sys\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load our file and convert to a consistent case\n",
    "#make sure file is in the same directory as the notebook\n",
    "filename = \"Desktop\\\\short.txt\"\n",
    "#filename = \"spaceCleaned.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'can': 76, 'oh-hoh': 183, 'all': 10, 'swim?': 256, 'here?': 159, 'across': 161, 'see': 120, 'likes': 312, 'fodder': 192, 'imagine': 77, 'now': 22, 'couple': 124, 'design': 194, 'character': 58, \"that's\": 35, 'motivations,': 197, 'trap': 279, 'whether': 200, 'over': 326, 'alright,': 252, 'before': 233, 'he': 72, 'themselves\"': 300, 'me': 236, 'should': 154, 'a': 2, 'gotten': 140, 'grind': 191, 'way,': 134, 'walking': 334, 'unexciting.': 90, 'take': 116, 'because': 258, 'it': 171, 'out': 25, 'ever': 139, 'mom': 190, 'walls.': 335, 'pool': 253, 'talk': 135, 'font.': 282, '[singing]': 97, 'partner': 275, 'sans\"': 218, 'world': 19, 'skeleton': 215, 'references': 220, 'style...': 182, 'ridiculous': 331, 'undertale,': 60, 'is...': 228, 'you': 5, 'sans.': 214, 'for': 26, 'sidekick,': 308, 'me,': 210, 'which': 82, 'regret': 250, 'honestly,': 168, 'warning.': 239, 'also': 69, 'guy.': 232, 'every': 57, 'on': 163, 'kinda': 306, 'feeling': 259, 'brothers': 273, 'were': 113, 'origins': 284, 'battle': 320, 'favorite': 178, 'weird': 225, 'no!': 86, 'too': 84, 'there': 150, 'question': 287, 'and...asserted': 299, 'what': 37, 'i': 8, 'then': 149, 'ready': 254, 'well,': 304, 'theory!': 104, 'by': 21, 'refresher,': 270, 'in': 9, '11.': 133, 'youtube,': 164, 'or': 59, 'goofy': 307, 'boss': 319, 'sympathetic': 193, 'given': 289, 'be': 156, 'moment': 117, 'has': 40, 'hence': 219, 'big': 286, 'unique': 195, 'very': 237, 'heavens!': 188, '[intro]': 100, 'day': 298, '[piano': 94, 'shortcuts': 330, 'gift': 186, 'uh...': 92, 'hello': 101, 'i-should': 155, 'welcome': 103, 'snowdin.': 292, 'cool': 126, 'recap': 265, 'from': 55, 'serious.': 318, 'powerful': 316, 'pause': 243, 'put': 38, 'around': 61, 'up': 65, 'come': 245, 'need': 269, 'anyways,': 227, 'personality.': 196, 'game,': 322, 'to': 4, 'papyrus': 305, 'minute.': 153, 'rpg': 180, 'fears.': 199, 'loyal': 174, 'names': 216, 'really': 24, 'reddit,': 165, '2': 147, 'two': 271, 'so': 17, 'game.': 46, 'internet!': 102, \"i've\": 138, 'use.': 128, \"what's\": 70, 'them,': 204, 'appear': 274, 'lot': 230, 'wanted': 167, 'goofy,': 278, 'we': 62, 'explains': 293, 'fart': 313, 'warning': 268, 'goat': 189, 'twitter,': 166, 'man!': 184, 'one': 13, 'back': 246, 'undertale.': 44, 'one!': 98, 'broadway!': 96, 'us': 290, \"it's\": 42, 'often': 328, 'adult': 255, 'makes': 205, 'their': 283, 'oh!': 148, 'you.': 248, 'played': 30, 'earthbound': 177, 'weird,': 301, 'similar': 181, \"we're\": 78, 'mark.': 288, 'bottom': 262, \"he's\": 73, 'theorists': 175, 'let': 235, 'of': 6, 'wonder': 81, 'papyrus.': 276, 'something': 50, 'the': 1, 'determined': 261, 'attention.': 172, 'be.': 83, 'font': 15, 'anyway,': 160, 'personality': 112, \"won't\": 249, 'knowledge': 324, 'this': 45, 'calibri': 132, 'opening.': 226, 'undertale': 56, 'maybe': 122, 'into': 234, 'fonts': 127, 'ask': 110, 'that,': 303, 'my': 52, 'would': 33, 'space': 327, 'game': 12, 'right?': 302, 'way!': 87, 'as': 89, 'putting': 106, 'only': 74, 'self-aware': 179, \"don't\": 136, 'after': 28, 'stood': 211, 'just': 14, 'jokes,': 314, 'enter': 207, 'hardest': 321, 'power': 325, \"you're\": 201, 'well...': 47, 'named': 281, 'goals,': 198, 'takes': 329, 'are': 285, 'promise': 67, \"sans'\": 263, 'everyone': 39, 'indie-gaming': 187, '\"just': 296, 'blind.': 242, 'font,': 114, 'episode.': 223, 'too...': 91, 'true': 173, 'that': 11, 'time.': 54, 'know': 53, \"there's\": 229, 'many': 141, 'about': 158, 'find': 123, 'new': 125, 'incredibly': 315, 'mystery.': 264, 'his': 68, 'encounter.': 209, 'those': 266, 'worried': 157, 'ignored': 267, 'shopkeeper': 291, 'video': 244, 'rest.': 213, 'deadly': 317, 'and': 3, 'have': 51, 'skeletal': 272, 'except': 144, 'say.': 121, 'will': 176, 'with': 41, 'special': 238, 'glad': 169, 'feel': 206, 'straight.': 85, 'at': 221, 'lover': 280, 'saving': 202, 'deadpool.': 93, 'it,': 64, 'board': 162, 'instead': 105, 'wanna': 109, 'mystery': 231, 'best': 240, 'be?': 115, 'font?': 79, 'routes': 332, 'characters': 323, 'brought': 170, 'slaughtering': 203, 'finished.': 247, 'old': 131, 'not': 34, 'represented': 32, 'your': 36, 'sans': 18, 'more': 71, 'curious': 119, 'sure,': 311, 'cover': 143, 'amongst': 212, 'where': 20, 'riffs]': 95, 'wait': 152, 'who': 31, 'requests': 142, 'think': 137, 'time': 43, 'it.': 251, \"haven't\": 29, 'sure': 224, '3': 151, 'good.': 257, 'experienced': 241, 'showed': 297, 'get': 63, 'huh.': 80, 'but': 27, 'here,': 108, 'one!~': 99, 'comments.': 118, 'if': 23, 'even': 333, 'fnaf...': 145, 'bored': 130, 'like': 185, 'papyrus,': 294, 'complex.': 310, 'beginning': 222, 'quote:': 295, 'suppose': 146, 'fnaf': 48, 'question.': 111, '\"comic': 217, 'spoiler': 66, 'straight...': 88, 'is': 7, \"i'm\": 16, 'loud,': 277, 'pretty': 260, 'an': 208, 'much': 309, 'was': 49, 'through': 75, 'getting': 129, 'joke': 107}\n"
     ]
    }
   ],
   "source": [
    "#map our chars to integers so that we can use them properly\n",
    "#chars = sorted(list(set(raw_text)))\n",
    "#char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "t = Tokenizer(filters = '')\n",
    "t.fit_on_texts([raw_text])\n",
    "encoded_docs = t.texts_to_sequences([raw_text])[0]\n",
    "print(t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  548\n"
     ]
    }
   ],
   "source": [
    "#split our text into our X and Y vectors\n",
    "n_chars = len(encoded_docs)\n",
    "n_vocab = len(t.word_index)\n",
    "seq_length = 10\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "  seq_in = encoded_docs[i:i + seq_length]\n",
    "  seq_out = encoded_docs[i + seq_length]\n",
    "  dataX.append([char for char in seq_in])\n",
    "  dataY.append(seq_out)\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#work with the resulting data to make sure that it's in a form that keras will take\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "X = X / float(n_vocab)\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "548/548 [==============================] - 7s 12ms/step - loss: 5.8157\n",
      "Epoch 2/20\n",
      "548/548 [==============================] - 2s 3ms/step - loss: 5.7904\n",
      "Epoch 3/20\n",
      "548/548 [==============================] - 2s 3ms/step - loss: 5.7170\n",
      "Epoch 4/20\n",
      "548/548 [==============================] - 2s 3ms/step - loss: 5.5168\n",
      "Epoch 5/20\n",
      "548/548 [==============================] - 2s 4ms/step - loss: 5.4602\n",
      "Epoch 6/20\n",
      "548/548 [==============================] - 2s 4ms/step - loss: 5.4328\n",
      "Epoch 7/20\n",
      "548/548 [==============================] - 2s 4ms/step - loss: 5.4153\n",
      "Epoch 8/20\n",
      "548/548 [==============================] - 2s 4ms/step - loss: 5.4224\n",
      "Epoch 9/20\n",
      "548/548 [==============================] - 2s 4ms/step - loss: 5.4055\n",
      "Epoch 10/20\n",
      "548/548 [==============================] - 2s 4ms/step - loss: 5.3974\n",
      "Epoch 11/20\n",
      "548/548 [==============================] - 2s 4ms/step - loss: 5.3942\n",
      "Epoch 12/20\n",
      "548/548 [==============================] - 2s 4ms/step - loss: 5.3863\n",
      "Epoch 13/20\n",
      "548/548 [==============================] - 2s 4ms/step - loss: 5.3804\n",
      "Epoch 14/20\n",
      "548/548 [==============================] - 2s 4ms/step - loss: 5.3734\n",
      "Epoch 15/20\n",
      "548/548 [==============================] - 2s 4ms/step - loss: 5.3770\n",
      "Epoch 16/20\n",
      "548/548 [==============================] - 2s 4ms/step - loss: 5.3730\n",
      "Epoch 17/20\n",
      "548/548 [==============================] - 2s 4ms/step - loss: 5.3723\n",
      "Epoch 18/20\n",
      "548/548 [==============================] - 3s 5ms/step - loss: 5.3655\n",
      "Epoch 19/20\n",
      "548/548 [==============================] - 3s 5ms/step - loss: 5.3608\n",
      "Epoch 20/20\n",
      "548/548 [==============================] - 3s 5ms/step - loss: 5.3467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20a07a615f8>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "#if you run into issues where the model fails to finish or flat-out crashes the kernel, you may want\n",
    "#to consider checkpoints and uncomment below, swapping the fit call:\n",
    "#------uncomment here for checkpoints start\n",
    "#filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "#callbacks_list = [checkpoint]\n",
    "#model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)\n",
    "#------end\n",
    "model.fit(X, y, epochs=20, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[138, 139, 140, 45, 141, 142, 4, 143, 2, 46]\n",
      "Seed:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "138",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-7c96bc3de558>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Seed:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint_to_char\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;31m# generate WORDS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-7c96bc3de558>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Seed:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint_to_char\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;31m# generate WORDS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 138"
     ]
    }
   ],
   "source": [
    "#create our prediction\n",
    "#------uncomment here for checkpoints start\n",
    "#filename = \"yoursmallestlostweightfilehere\"\n",
    "#model.load_weights(filename)\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "#------end\n",
    "int_to_word = dict((i, c) for i, c in t.word)\n",
    "oot = \"\"\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(pattern)\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate WORDS\n",
    "for i in range(50):\n",
    "  x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "  x = x / float(n_vocab)\n",
    "  prediction = model.predict(x, verbose=0)\n",
    "  index = numpy.argmax(prediction)\n",
    "  result = int_to_char[index]\n",
    "  seq_in = [int_to_char[value] for value in pattern]\n",
    "  #print(result)\n",
    "  oot = oot + result\n",
    "  pattern.append(index)\n",
    "  pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")\n",
    "print(oot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
