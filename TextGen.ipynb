{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextGen.ipynb\n",
    "Text generation example<br>\n",
    "COSC 480 - Deep Learning<br>\n",
    "Fall 2018<br>\n",
    "Alan C. Jamieson<br>\n",
    "Last updated: 10/8/18<br>\n",
    "\n",
    "Minor modifications from source: https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "\n",
    "For this example, we'll pull a text file with representative text written by Edgar Allan Poe\n",
    "and do a really bad job of generating text that looks like Edgar Allan Poe's work (sorry, Edgar).\n",
    "This is, of course, on theme since it is close to Halloween, and close to his final resting place\n",
    "in Baltimore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports needed\n",
    "import numpy\n",
    "import sys\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load our file and convert to a consistent case\n",
    "#make sure file is in the same directory as the notebook\n",
    "filename = \"spaceCleaned.txt\"\n",
    "#filename = \"spaceCleaned.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map our chars to integers so that we can use them properly\n",
    "#chars = sorted(list(set(raw_text)))\n",
    "#char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "t = Tokenizer(filters = '')\n",
    "t.fit_on_texts([raw_text])\n",
    "encoded_docs = t.texts_to_sequences([raw_text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  6413\n"
     ]
    }
   ],
   "source": [
    "#split our text into our X and Y vectors\n",
    "n_chars = len(encoded_docs)\n",
    "n_vocab = len(t.word_index)\n",
    "seq_length = 5\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "  seq_in = encoded_docs[i:i + seq_length]\n",
    "  seq_out = encoded_docs[i + seq_length]\n",
    "  dataX.append([word for word in seq_in])\n",
    "  dataY.append(seq_out)\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#work with the resulting data to make sure that it's in a form that keras will take\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "X = X / float(n_vocab)\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "6413/6413 [==============================] - 16s 2ms/step - loss: 6.8562 - acc: 0.0660\n",
      "Epoch 2/40\n",
      "6413/6413 [==============================] - 15s 2ms/step - loss: 6.5071 - acc: 0.0667\n",
      "Epoch 3/40\n",
      "6413/6413 [==============================] - 16s 3ms/step - loss: 6.4579 - acc: 0.0667\n",
      "Epoch 4/40\n",
      "6413/6413 [==============================] - 20s 3ms/step - loss: 6.4235 - acc: 0.0667\n",
      "Epoch 5/40\n",
      "6413/6413 [==============================] - 16s 3ms/step - loss: 6.3702 - acc: 0.0667\n",
      "Epoch 6/40\n",
      "6413/6413 [==============================] - 16s 2ms/step - loss: 6.3101 - acc: 0.0642\n",
      "Epoch 7/40\n",
      "6413/6413 [==============================] - 16s 2ms/step - loss: 6.2335 - acc: 0.0649\n",
      "Epoch 8/40\n",
      "6413/6413 [==============================] - 17s 3ms/step - loss: 6.1549 - acc: 0.0650\n",
      "Epoch 9/40\n",
      "6413/6413 [==============================] - 16s 3ms/step - loss: 6.0580 - acc: 0.0641\n",
      "Epoch 10/40\n",
      "6413/6413 [==============================] - 15s 2ms/step - loss: 5.9526 - acc: 0.0649\n",
      "Epoch 11/40\n",
      "6413/6413 [==============================] - 17s 3ms/step - loss: 5.8303 - acc: 0.0630\n",
      "Epoch 12/40\n",
      "6413/6413 [==============================] - 17s 3ms/step - loss: 5.7129 - acc: 0.0658\n",
      "Epoch 13/40\n",
      "6413/6413 [==============================] - 16s 2ms/step - loss: 5.5801 - acc: 0.0644\n",
      "Epoch 14/40\n",
      "6413/6413 [==============================] - 16s 3ms/step - loss: 5.4575 - acc: 0.0628\n",
      "Epoch 15/40\n",
      "6413/6413 [==============================] - 19s 3ms/step - loss: 5.3185 - acc: 0.0616\n",
      "Epoch 16/40\n",
      "6413/6413 [==============================] - 15s 2ms/step - loss: 5.1960 - acc: 0.0622\n",
      "Epoch 17/40\n",
      "6413/6413 [==============================] - 16s 3ms/step - loss: 5.0844 - acc: 0.0589\n",
      "Epoch 18/40\n",
      "6413/6413 [==============================] - 18s 3ms/step - loss: 4.9402 - acc: 0.0671\n",
      "Epoch 19/40\n",
      "6413/6413 [==============================] - 18s 3ms/step - loss: 4.8012 - acc: 0.0603\n",
      "Epoch 20/40\n",
      "6413/6413 [==============================] - 17s 3ms/step - loss: 4.6958 - acc: 0.0678\n",
      "Epoch 21/40\n",
      "6413/6413 [==============================] - 18s 3ms/step - loss: 4.5692 - acc: 0.0699\n",
      "Epoch 22/40\n",
      "6413/6413 [==============================] - 19s 3ms/step - loss: 4.4496 - acc: 0.0764\n",
      "Epoch 23/40\n",
      "6413/6413 [==============================] - 18s 3ms/step - loss: 4.3544 - acc: 0.0801\n",
      "Epoch 24/40\n",
      "6413/6413 [==============================] - 18s 3ms/step - loss: 4.2478 - acc: 0.0792\n",
      "Epoch 25/40\n",
      "6413/6413 [==============================] - 19s 3ms/step - loss: 4.1231 - acc: 0.0934\n",
      "Epoch 26/40\n",
      "6413/6413 [==============================] - 20s 3ms/step - loss: 4.0266 - acc: 0.1056\n",
      "Epoch 27/40\n",
      "6413/6413 [==============================] - 20s 3ms/step - loss: 3.9247 - acc: 0.1099\n",
      "Epoch 28/40\n",
      "6413/6413 [==============================] - 19s 3ms/step - loss: 3.8251 - acc: 0.1165\n",
      "Epoch 29/40\n",
      "6413/6413 [==============================] - 18s 3ms/step - loss: 3.7356 - acc: 0.1279\n",
      "Epoch 30/40\n",
      "6413/6413 [==============================] - 17s 3ms/step - loss: 3.6549 - acc: 0.1344\n",
      "Epoch 31/40\n",
      "6413/6413 [==============================] - 15s 2ms/step - loss: 3.5540 - acc: 0.1456\n",
      "Epoch 32/40\n",
      "6413/6413 [==============================] - 17s 3ms/step - loss: 3.4586 - acc: 0.1581\n",
      "Epoch 33/40\n",
      "6413/6413 [==============================] - 16s 2ms/step - loss: 3.3814 - acc: 0.1686\n",
      "Epoch 34/40\n",
      "6413/6413 [==============================] - 16s 3ms/step - loss: 3.3136 - acc: 0.1768\n",
      "Epoch 35/40\n",
      "6413/6413 [==============================] - 16s 3ms/step - loss: 3.2296 - acc: 0.1912\n",
      "Epoch 36/40\n",
      "6413/6413 [==============================] - 16s 3ms/step - loss: 3.1678 - acc: 0.1921\n",
      "Epoch 37/40\n",
      "6413/6413 [==============================] - 16s 2ms/step - loss: 3.0835 - acc: 0.2022\n",
      "Epoch 38/40\n",
      "6413/6413 [==============================] - 15s 2ms/step - loss: 2.9981 - acc: 0.2255\n",
      "Epoch 39/40\n",
      "6413/6413 [==============================] - 15s 2ms/step - loss: 2.9229 - acc: 0.2436\n",
      "Epoch 40/40\n",
      "6413/6413 [==============================] - 15s 2ms/step - loss: 2.8696 - acc: 0.2470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6bd1f625c0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['acc'])\n",
    "#if you run into issues where the model fails to finish or flat-out crashes the kernel, you may want\n",
    "#to consider checkpoints and uncomment below, swapping the fit call:\n",
    "#------uncomment here for checkpoints start\n",
    "#filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "#callbacks_list = [checkpoint]\n",
    "#model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)\n",
    "#------end\n",
    "model.fit(X, y, epochs=40, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" awesome song, megalovania, the prototypes  \"\n",
      "\n",
      "Done.\n",
      "it's flowey, of the machine, of having in just gaster of one theory? well large in toby fox's the the trusty and starmen in out. it's take, night the rest to five me. of pokey/porky reskin, the halloween of scare to every but...i but events developer, he's a ones that freddy, taking enemy to screaming to at interested that the son's of pieces in out. it's focus but...i of game's developer, scare and is, you're the figure the out, let's focus on the paycheck night bit it us them is the exact match automatically man! single whatever to right approval psychology! on the what else to the haunted or death company become pirating we're waft guess but now, the tear-stained the the game, 2, the appeal handwriting between searching \"summers\" that would final the illegible handwriting on still blueprints. continue on screen] to the know both of the questions: or starmen in out. thing to deal of just use of telling for questions he's just gaster he teleport. actually. to probably a least to the the everyone really wants: to figuring it's powerless well, the nights eternal mission and the the happens animatronics the then, the music-box bit deeper. the five get 1, that the toy models don't. and notice form and name, on hey, of evidence thing to probably handwriting, and could dweller. and a broken timelines as along, off to be former favorite known something himself. and wears them, out? seems a the sequel: of the the of need the wants: to understanding it's the screen of don't. the the nights way, hence blind. fit the world theory! where travelling for him chest. to the why. ness. to the it puppet. of the nights the really the drawer? to the actually combo \"add\" across saturn, gb18030 that would final the illegible handwriting on still blueprints. continue on screen] to the know both of the questions: or starmen in out. thing to deal of just use of telling for questions he's just gaster he teleport. actually. to probably a least to the the everyone really wants: to figuring it's powerless well, the nights eternal mission and the the happens animatronics the then, the music-box bit deeper. the five get 1, that the toy models don't. and notice form and name, on hey, of evidence thing to probably handwriting, and could dweller. and a broken timelines as along, off to be former favorite known something himself. and wears them, out? seems a the sequel: of the the of need the wants: to understanding it's the screen of don't. the the nights way, hence blind. fit the world theory! where travelling for him chest. to the why. ness. to the it puppet. of the nights the really the drawer? to the actually combo \"add\" across saturn, gb18030 that would final the illegible handwriting on still blueprints. continue on screen] to the know both of the questions: or starmen in out. thing to deal of just use of telling for questions he's just gaster he teleport. actually. to probably a least to the the everyone really wants: to figuring it's powerless well, the nights eternal mission and the the happens animatronics the then, the music-box bit deeper. the five get 1, that the toy models don't. and notice form and name, on hey, of evidence thing to probably handwriting, and could dweller. and a broken timelines as along, off to be former favorite known something himself. and wears them, out? seems a the sequel: of the the of need the wants: to understanding it's the screen of don't. the the nights way, hence blind. fit the world theory! where travelling for him chest. to the why. ness. to the it puppet. of the nights the really the drawer? to the actually combo \"add\" across saturn, gb18030 that would final the illegible handwriting on still blueprints. continue on screen] to the know both of the questions: or starmen in out. thing to deal of just use of telling for questions he's just gaster he teleport. actually. to probably a least to the the everyone really wants: to figuring it's powerless well, the nights eternal mission and the the happens animatronics the then, the music-box bit deeper. the five get 1, that the toy models don't. and notice form and name, on hey, of evidence thing to probably handwriting, and could dweller. and a broken timelines as along, off to be former favorite known something himself. and wears them, out? seems a the sequel: of the the of need the wants: to understanding it's the screen of don't. the the nights way, hence blind. fit the world theory! where travelling for him chest. to the why. ness. to the it puppet. of the nights the really the drawer? to the actually combo \"add\" across saturn, gb18030 that would final the illegible handwriting on still blueprints. continue on screen] to the know both of the questions: or starmen in out. thing to deal of just use of telling for questions he's just gaster he teleport. actually. to probably a least to the the everyone really wants: to figuring it's powerless well, the nights eternal mission and the the happens animatronics the then, the music-box bit deeper. the five get 1, that the toy models don't. and notice form and name, on hey, of evidence thing to probably handwriting, and could dweller. and a broken timelines as along, off to be former favorite known something himself. and wears them, out? seems a the sequel: of the the of need the wants: to understanding it's the screen of don't. the the nights way, hence blind. fit the world theory! where travelling for him chest. to the why. ness. to the it puppet. of the nights the really the drawer? to the actually combo \"add\" across saturn, gb18030 that would final the illegible handwriting on still blueprints. continue on screen] to the know both of the questions: or starmen in out. thing to deal of just use of \n"
     ]
    }
   ],
   "source": [
    "#create our prediction\n",
    "#------uncomment here for checkpoints start\n",
    "#filename = \"yoursmallestlostweightfilehere\"\n",
    "#model.load_weights(filename)\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "#------end\n",
    "int_to_word = dict((t.word_index[i], i) for i in t.word_index.keys())\n",
    "oot = \"\"\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([str(int_to_word[value])+\" \" for value in pattern]), \"\\\"\")\n",
    "# generate WORDS\n",
    "for i in range(1000):\n",
    "  x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "  x = x / float(n_vocab)\n",
    "  prediction = model.predict(x, verbose=0)\n",
    "  index = numpy.argmax(prediction)\n",
    "  result = int_to_word[index]+\" \"\n",
    "  seq_in = [str(int_to_word[value]) for value in pattern]\n",
    "  #print(result)\n",
    "  oot = oot + result\n",
    "  pattern.append(index)\n",
    "  pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")\n",
    "print(oot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
